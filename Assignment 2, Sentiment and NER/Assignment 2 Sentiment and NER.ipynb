{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07b5c40-83c8-4d18-8f30-41a79d642b4e",
   "metadata": {},
   "source": [
    "# Assignment 2: Sentiment and NER\n",
    "#### By Line Stampe-Degn Møller\n",
    "\n",
    "##### Assignment and data:\n",
    "https://github.com/CDS-AU-DK\n",
    "\n",
    "https://github.com/CDS-AU-DK/cds-language/blob/main/assignments/assignment2.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc8695-d591-4bbf-a945-96f845822e9c",
   "metadata": {},
   "source": [
    "Packages used:\n",
    "- tqdm\n",
    "- spaCy\n",
    "- python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab9c5a1-12d9-4ec2-b02d-e3325850bacb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T02:18:30.809307Z",
     "iopub.status.busy": "2022-03-16T02:18:30.809170Z",
     "iopub.status.idle": "2022-03-16T02:18:32.449498Z",
     "shell.execute_reply": "2022-03-16T02:18:32.448799Z",
     "shell.execute_reply.started": "2022-03-16T02:18:30.809291Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "# Data analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# sentiment analysis VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# sentiment with spacyTextBlob\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "# visualisations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e35b481-90a8-42c3-9eb2-c2670a6ddd0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T02:18:32.453308Z",
     "iopub.status.busy": "2022-03-16T02:18:32.453167Z",
     "iopub.status.idle": "2022-03-16T02:18:32.462935Z",
     "shell.execute_reply": "2022-03-16T02:18:32.462512Z",
     "shell.execute_reply.started": "2022-03-16T02:18:32.453292Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../CDS-LANG/100_english_novels/corpus/Eliot_Adam_1859.txt',\n",
       " '../CDS-LANG/100_english_novels/corpus/Bennet_Helen_1910.txt',\n",
       " '../CDS-LANG/100_english_novels/corpus/Conrad_Rover_1923.txt',\n",
       " '../CDS-LANG/100_english_novels/corpus/Forster_Angels_1905.txt',\n",
       " '../CDS-LANG/100_english_novels/corpus/Gaskell_Wives_1865.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "# get the filepath\n",
    "filepath = os.path.join(\"..\", \"CDS-LANG\", \"100_english_novels\", \"corpus\")\n",
    "novels = os.listdir(filepath)\n",
    "                    \n",
    "import re\n",
    "\n",
    "all_novels = []\n",
    "\n",
    "# I tried to remove the two novels that start with a \".\" bc it seemed like those novels created problems. \n",
    "# However, I wasn't succesfull in my attempt, so my temporary solution is to only load the first 79 novels \n",
    "# (cap it right before the first novel with a \".\" in the beginning of the file name).\n",
    "# for novel in novels:\n",
    "#    match = re.match(\"\\.\", string)\n",
    "#    if match:\n",
    "#        pass\n",
    "#    else:\n",
    "#        all_filepaths = os.path.join(filepath, novel)\n",
    "#        all_novels.append(all_filepaths)\n",
    "\n",
    "for novel in novels[0:78]:\n",
    "    all_filepaths = os.path.join(filepath, novel)\n",
    "    all_novels.append(all_filepaths)\n",
    "    \n",
    "all_novels[0:5]  # Print the first 5 to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a126de51-e665-4fa9-82b7-da5cef3cbe62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T02:20:47.786122Z",
     "iopub.status.busy": "2022-03-16T02:20:47.785596Z",
     "iopub.status.idle": "2022-03-16T02:20:47.793707Z",
     "shell.execute_reply": "2022-03-16T02:20:47.792693Z",
     "shell.execute_reply.started": "2022-03-16T02:20:47.786072Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentiment_plot(novel):\n",
    "    with open(novel, \"r\", encoding=\"utf-8\") as file:\n",
    "        novel_content = file.read()\n",
    "    nlp.max_length = 1500000\n",
    "    doc = nlp(novel_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d52e879-5ac9-4fed-9f24-1997b1f6cd65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T02:20:48.880406Z",
     "iopub.status.busy": "2022-03-16T02:20:48.879882Z",
     "iopub.status.idle": "2022-03-16T02:38:39.201051Z",
     "shell.execute_reply": "2022-03-16T02:38:39.200005Z",
     "shell.execute_reply.started": "2022-03-16T02:20:48.880356Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 12/78 [17:50<1:38:06, 89.19s/it] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 2544074 exceeds maximum of 1500000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_627/2607285842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnovel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_novels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msentiment_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnovel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_627/1954024965.py\u001b[0m in \u001b[0;36msentiment_plot\u001b[0;34m(novel)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mnovel_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnovel_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \"\"\"\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE866\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \"\"\"\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 2544074 exceeds maximum of 1500000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "for novel in tqdm(all_novels):\n",
    "    sentiment_plot(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b13e7b-2a58-4a4a-adb2-614c05206b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
